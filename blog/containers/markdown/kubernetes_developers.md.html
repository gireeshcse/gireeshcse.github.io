<h1 id="containers">Containers</h1>
<h2 id="benefits">Benefits</h2>
<ul>
<li>Accelerate Developer Onboarding </li>
<li>Eliminate App Conflicts </li>
<li>Environment Consistency</li>
<li>Ship Software Faster</li>
</ul>
<h1 id="kubernetes">Kubernetes</h1>
<p>Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications.</p>
<h2 id="advantages">Advantages</h2>
<ul>
<li>We can package an app and we can let kubernetes to manage it for us</li>
<li>Management of containers</li>
<li>Elimination of single points of failures</li>
<li>Scales containers</li>
<li>Updates containers without bringing down the application.</li>
<li>Have a robust networking and persistent storage options.</li>
</ul>
<p><strong>Conductor of containers</strong></p>
<p><strong>Provides a declarative way to define a cluster&#39; state</strong></p>
<p><strong>Contains one or more master nodes and worker nodes(can be Physical servers,VMs).The workers nodes contains PODS which contains containers</strong></p>
<ul>
<li>POD         ----&gt; Suit</li>
<li>Container   ----&gt; Person</li>
<li>Store(etcd)(acts as a database for cluster), </li>
<li>controller manager(Takes requests and uses scheduler to perform/act upon actions),</li>
<li>API Server(To interact with the cluster to give instruction to go from one state to other&lt;--kubectl)</li>
</ul>
<p><strong>Each node has a Kubelet to communicate with the master,Container runtime to run containers within the PODs and  Kube-Proxy ensures each pod has a IP address</strong></p>
<h2 id="benefits-1">Benefits</h2>
<ul>
<li>Orchestrate Containers</li>
<li>Zero-Downtime Deployements </li>
<li>Self Healings</li>
<li>Scale Containers</li>
</ul>
<h3 id="for-developers">For Developers</h3>
<ul>
<li>Emulate production locally</li>
<li>Move from Docker Compose to Kubernetes</li>
<li>Create an end-to-end testing environment</li>
<li>To ensure application scales properly</li>
<li>To ensure secrets/config are working properly.</li>
<li>Performance testing scenarios</li>
<li>Workload scenarios(CI/CD and more)</li>
<li>Helps in learning how to leverage deployment options</li>
<li>We can Help DevOps create resources and solve problems</li>
</ul>
<h2 id="running-locally">Running Locally</h2>
<ul>
<li>Install <a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Minikude</a> </li>
<li><a href="https://www.docker.com/products/docker-desktop">Docker Desktop</a> available for Mac and Windows.</li>
</ul>
<p>Note: (To use KVM driver for ubuntu 18.04)(<a href="https://www.linuxtechi.com/install-configure-kvm-ubuntu-18-04-server/">https://www.linuxtechi.com/install-configure-kvm-ubuntu-18-04-server/</a>)</p>
<p><a href="https://minikube.sigs.k8s.io/docs/reference/drivers/">Minikube drivers</a></p>
<p>Note: sudo minikube start --vm-driver=none for Ubuntu 18.04 minikube version 1.6.2 worked <a href="https://github.com/kubernetes/minikube/releases/">minikube release</a></p>
<ul>
<li><p>To install latest minikube (Linux) Installed Binary</p>
<p>  curl -Lo minikube <a href="https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64">https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64</a> <br>  &amp;&amp; chmod +x minikube</p>
<p>  sudo install minikube /usr/local/bin/</p>
</li>
</ul>
<h2 id="minikube-commandsgenerally-requires-sudo">minikube Commands(Generally requires sudo)</h2>
<pre><code># To start a local kubernetes cluster Generally run with 
# sudo minikube --vm-driver=none if running in host directly not in VM
minikube start 
minikube stop # stop a local kubernetes cluster
minikube dashboard # access the kubernetes dashboard running within the minikube cluster
minikube delete # deletes a local kubernetes cluster
minikube start -p &lt;name&gt;&#39; to create a new cluster, or &#39;minikube delete&#39; to delete this one
minikube status</code></pre><h2 id="kubectl-commands">kubectl commands</h2>
<pre><code>    kubectl version 
    kubectl cluster-info
    kubectl get all # all info about Kubernetes Pods,Deployments,Services, and more
    kubectl run [container-name]  --image=[image-name] # simple way to create a deployment for a POD
    kubectl port-forward [pod] [ports] # forward a port to allow external access
    kubectl expose [port] # expose a port for a Deployment/Pod
    kubectl create [resource]  # create a resource
    kubectl apply [resource]  # createor modify a resource 
    kubectl --help
    kubectl get pods
    kubectl get services</code></pre><h2 id="enabling-web-ui-dashboard">Enabling Web UI Dashboard</h2>
<p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">For more Info</a></p>
<pre><code>    sudo minkube dashboard 

    or

    kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/\
    v2.0.0-beta8/aio/deploy/recommended.yaml
    kubectl describe secret -n kube-system</code></pre><p>From above command copy the token of type <strong>kubernetes.io/service-account-token</strong></p>
<pre><code>    kubectl proxy</code></pre><h3 id="error--first-record-does-not-look-like-a-tls-handshake-kubernetes">Error:  first record does not look like a tls handshake kubernetes</h3>
<p>Change (https to http)</p>
<p><a href="http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</a></p>
<p>to </p>
<p><a href="http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/</a></p>
<h2 id="featues">Featues</h2>
<ul>
<li><p>Service discovery and load balancing </p>
<p>  Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.</p>
</li>
<li><p>Storage Orchestration</p>
<p>  Automatically mount the storage system of our choice, whether from local storage, a public cloud provider such as GCP or AWS, or a network storage system such as NFS, iSCSI, Gluster, Ceph, Cinder, or Flocker.</p>
</li>
<li><p>Self Healing </p>
<p>  Restarts containers that fail, replaces and reschedules containers when nodes die, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve.</p>
</li>
<li><p>Automating rollouts and rollbacks</p>
<p>  Kubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn’t kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.</p>
</li>
<li><p>Secret and configuration management</p>
<p>  Deploy and update secrets and application configuration without rebuilding our image and without exposing secrets in your stack configuration.</p>
</li>
<li><p>Horizontal scaling </p>
<p>  Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage.</p>
</li>
</ul>
<h1 id="pods">Pods</h1>
<ul>
<li><p>A Pod is the basic execution unit of a Kubernetes application-the smallest and simplest unit in the Kubernetes object model that you create or deploy.</p>
</li>
<li><p>Pods run containers</p>
</li>
<li><p>Pods acts as a environment for containers.</p>
</li>
<li><p>As a Developer we need to organise the application &quot;parts&quot; into <strong>Pods</strong> (Server, caching, APIs, database, etc.)</p>
</li>
<li><p>Pod IP, memory, volumes, etc. shared across containers</p>
</li>
<li><p>we can scale horizontally by adding Pod replicas</p>
</li>
<li><p><strong>Pods live and die but never come back to life.</strong> New one is created</p>
</li>
<li><p>Pod containers share the same Network namespace(share IP/port)</p>
</li>
<li><p>Pod containers have the same loopback network interfaces(localhost)</p>
</li>
<li><p>Containers processes need to bind to different ports within a Pod</p>
</li>
<li><p>Ports can be reused by containers in separate Pods</p>
</li>
<li><p>Pods never span nodes</p>
</li>
</ul>
<p>Note: Pods have different ips (10.0.0.33, 10.0.0.43)</p>
<h2 id="running-a-pod">Running a Pod</h2>
<p>Use any of the following</p>
<ul>
<li><p>kubectl run command</p>
</li>
<li><p>kubectl create/apply command with a yaml file.</p>
<pre><code>  # Run the nginx:alpine container in a Pod
  kubectl run [podname]  --image=nginx:alpine
  kubectl create deployment [podname] --image=nginx:alpine

  # Examples
  kubectl run sample-nginx-alpine-pod --image=nginx:alpine # deprecated
  kubectl create deployment nginx-pod --image=nginx:alpine

  # list only pods
  kubectl get pods
  kubectl get pods --watch
  # list all resources 
  kubectl get all</code></pre></li>
</ul>
<p><strong>Pods and containers are only accessible within the kubernetes cluster by default</strong></p>
<p><strong>One way to expose a container port externally: kubectl port-forward</strong></p>
<p><strong>Image to run a pod is a docker image</strong></p>
<pre><code>    # Enable Pod Container to be 
    # called externally
    kubectl port-forward [name-of-pod]  external_port:internal_port
    kubectl port-forward pod/nginx-pod-6fc99f67cd-h4zxr  8001:80 
    # 127.0.0.1:8001 to access the application

    # will cause pod to be recreated
    kubectl delete pod [name-of-pod]
    kubectl delete pod nginx-pod-6fc99f67cd-h4zxr

    # Delete Deployment that manages the pod
    # use kubectl get all to get deployment name of the pod
    kubectl delete deployment [name-of-deployment] 
    kubectl delete deployment nginx-pod</code></pre><h3 id="yaml-review">YAML Review</h3>
<ul>
<li>Composed of maps and lists</li>
<li>Indentation matters (be consistent!)</li>
<li>Always use spaces</li>
<li>Maps:<ul>
<li>name:value pairs</li>
<li>Maps can contain other maps for more complex data structures</li>
</ul>
</li>
<li>Lists:<ul>
<li>Sequence of items</li>
<li>Multipe maps can be defined in a list</li>
</ul>
</li>
</ul>
<h4 id="example">Example</h4>
<pre><code>key: value
complexMap:
    key1: value
    key2:
    subKey: value

items:
    - item1
    - item2
    itemsMap:
    - map1: value
        map1Prop: value
    - map2: value
        map2Prop: value</code></pre><h3 id="nginxpodyml">nginx.pod.yml</h3>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-nginx
spec:
  containers:
  - name: my-nginx
    image: nginx:alpine</code></pre><h3 id="creating-a-pod-using-yaml">creating a Pod using YAML</h3>
<pre><code># perform a trial create and validate the YAML (validate true is default)
kubectl create --filename file.pod.yml --dry-run --validate=true
kubectl create -f nginx.prod.yml --dry-run --validate=true

# Create a Pod from YAML
# Will error if Pod already exists
kubectl create -f file.pod.yml

# altenative way to create or apply changes to a
# Pod from YAML
kubectl apply -f file.pod.yml
kubectl apply -f nginx.prod.yml
# above command creates a warning 
# use --save-config when you what to use 
# kubectl apply in future
kubectl create -f file.prod.yml --save-config # Store current properties in resource&#39;s annotations</code></pre><p><strong>--save-config</strong> causes the resource&#39;s configuration settings to be saved in the <strong>annotations</strong>.Having this allows in-place changes to be made to a Pod in the future using <strong>kubectl apply</strong></p>
<p>kubectl edit or kubectl patch can also be used to change small or subset of changes to a Pod.</p>
<pre><code># delete a Pod
kubectl delete pod [name-of-pod]

# delete Pod using YAML file that created it
kubectl delete -f file.pod.yml
kubectl delete -f nginx.prod.yml</code></pre><h3 id="nginxpodyml-1">nginx.pod.yml</h3>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-nginx
  labels:
    app: nginx
    rel: stable
spec:
  containers:
  - name: my-nginx
    image: nginx:alpine
    ports:
    - containerPort: 80</code></pre><p>Note: <strong>labels are used in deployments</strong></p>
<pre><code>kubectl create -f nginx.pod.yml --save-config
# shows output in YAML this is because of --save-config(added annotations to the o/p)
kubectl get pod my-nginx -o yaml 
# To troubleshoot the Pod this output is useful
kubectl describe pod [pod-name]
kubectl describe pod my-nginx
kubectl apply -f nginx.pod.yml

# to go into pod with interactive shell
kubectl exec [pod-name] -it sh
kubectl exec my-nginx -it sh # enter exit the shell

kubectl edit -f nginx.pod.yml</code></pre><h3 id="error-socat-not-found---minikube">Error socat not found - minikube</h3>
<pre><code>kubectl port-forward my-nginx 8001:80</code></pre><p><a href="https://github.com/kubernetes/minikube/issues/68#issuecomment-344346923">Solution Link</a></p>
<p>if you&#39;re running the none driver, you&#39;ll need a whole host of dependencies that kubernetes requires: docker, iptables, socat, certain kernel modules enabled, etc</p>
<p>  sudo apt-get install socat # fixed the issue.</p>
<h3 id="pod-health">Pod health</h3>
<ul>
<li>Kubernetes relies on Probes to determine the health of a Pod container.</li>
<li>A Probe is a diagnostic performed periodically by the <strong>kubelet</strong> on a container.</li>
<li>Types of Probes<ul>
<li>Liveness Probes<ul>
<li>Used to determine if a Pod is healthy and running as expected.(Should a POD is to be restarted)</li>
</ul>
</li>
<li>Readiness probes <ul>
<li>Used to determine if a Pod should receive requests.(When the traffic has to be routed to the pod)</li>
</ul>
</li>
</ul>
</li>
<li>Failed Pod containers are recreated by default (restartPolicy defaults to Always)</li>
</ul>
<p>How to check the health of the probe.It depends on the container applications.We can execute direct actions on probes.Some of them are</p>
<ul>
<li>ExecAction - Executes an action inside the container</li>
<li>HTTPGetAction - HTTP GET request against container</li>
<li>TCPSocketAction - TCP check against the containers IP address on a specified port</li>
<li>Probes can have the following results <ul>
<li>Success</li>
<li>Failure</li>
<li>Unknown</li>
</ul>
</li>
</ul>
<h4 id="defining-an-http-liveness-probe">Defining an HTTP Liveness Probe</h4>
<p>Sample Requirements</p>
<ul>
<li>Check /index.html on port 80</li>
<li>Wait 15 seconds</li>
<li>Timeout after 2 seconds</li>
<li>Check every 5 seconds</li>
<li>Allow 1 failure before failing Pod</li>
</ul>
<h5 id="yaml-file">YAML File</h5>
<pre><code>    apiVersion: v1
    kind: Pod
    metadata:
      name: my-nginx
      labels:
        app: nginx
        rel: stable
    spec:
      containers:
      - name: my-nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /index.html
            port: 80
          initialDelaySeconds: 15
          timeoutSeconds: 2 # default is 1
          periodSeconds: 5 # Default is 10
          failureThreshold: 1 # Default is 3</code></pre><h4 id="defining-an-execaction-liveness-probe">defining an ExecAction Liveness Probe</h4>
<ul>
<li>Define args for container</li>
<li>Define liveness probe</li>
<li>Define action/command to execute</li>
</ul>
<h5 id="yaml-file-1">YAML File</h5>
<pre><code>    apiVersion: v1
    kind: Pod
    metadata:
      name: busybox-liveness-pod
    spec:
      containers:
      - name: busybox-liveness-pod
        image: k8s.gcr.io/busybox
        resources:
          limits:
            memory: &quot;64Mi&quot; # 64 MB
            cpu: &quot;50m&quot; # 50 millicpu (.05 cpu or 5% of the cpu)
         args:
         - /bin/sh
         - -c
         - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
        livenessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
           initialDelaySeconds: 5
           periodSeconds: 5</code></pre><p>The above one results in creation of new pods by deleting old pods after 30 seconds as /tmp/healthy file is removed.THis is repeated for new pods as well for this Pod created using above YAML file.</p>
<h4 id="defining-a-readiness-probe">Defining a Readiness Probe</h4>
<ul>
<li>Define readiness probe</li>
<li>Check /index.html on port 80</li>
<li>wait 2 seconds</li>
<li>Check every 5 seconds until the probe is up and runnning.</li>
</ul>
<h5 id="yaml-file-2">YAML File</h5>
<pre><code>    apiVersion: v1
    kind: Pod
    metadata:
      name: my-nginx
      labels:
        app: nginx
        rel: stable
    spec:
      containers:
      - name: my-nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /index.html
            port: 80
          initialDelaySeconds: 2
          periodSeconds: 5</code></pre><p><strong>Indetion is very important in YAML file. If any there is a problem it may result in unknown validation field error</strong></p>
<p><strong>Health checks provide a way to notify Kubernetes when a Pod has a problem</strong></p>
<h1 id="deployments">Deployments</h1>
<ul>
<li><p>A <strong>ReplicalSet</strong> is a declarative way to manage Pods.</p>
</li>
<li><p>A <strong>Deployment</strong> is a declarative way to manage Pods using a ReplicaSet.</p>
</li>
</ul>
<p>Deployments and ReplicaSets ensure Pods stay running and can be used to scale Pods.</p>
<h2 id="replicasets">ReplicaSets</h2>
<ul>
<li>Self-healing mechanism(Fault-tolerence)</li>
<li>Ensure the requested number of pods are available</li>
<li>Can be used to scale Pods (Horizontally)</li>
<li>Relies on a Pod template</li>
<li>No need to create Pods directly</li>
<li>Used by Deployments</li>
</ul>
<h2 id="deployment-manages-pods">Deployment manages Pods</h2>
<ul>
<li>Pods are managed using ReplicaSets</li>
<li>Scales ReplicaSets, which scale Pods</li>
<li>Supports zero-downtime updates by creating and destroying ReplicaSets</li>
<li>Provides rollback functionality</li>
<li>Creates a unique label that is assigned to the ReplicaSet and generated Pods</li>
<li>YAML is very similar to a ReplicaSet</li>
</ul>
<h3 id="defining-deployments-high-level">Defining Deployments (High-Level)</h3>
<pre><code>    apiVersion: apps/v1 # Kubernetes API version
    kind: Deployemnt  # Resource type
    metadata: # Metadata about the Deployment 
    spec:
      selector: # Select Pod template label(s)
      template: # template used to create the Pods
        spec:
          containers: # Containers that will run in the Pod.
          - name: my-nginx
            image: nginx:alpine</code></pre><h3 id="defining-a-deployment">Defining a Deployment</h3>
<ul>
<li><p>Kubernetes API version and resource type(Deployment)</p>
</li>
<li><p>Metadata about the Deployment </p>
</li>
<li><p>The Selector is used to &quot;select&quot; the template to use(based on labels)</p>
</li>
<li><p>Template to use to create the Pod/Containers(note that the selector matches the label)</p>
<pre><code>  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
    labels:
      app: my-nginx
      tier: frontend
  spec:
    selector:
      matchLabels:
        tier: frontend
    template:
      metadata:
        labels:
          tier: frontend
      spec:
        containers:
        - name: my-nginx
          image: nginx:alpine
          livenessProbe:
            httpGet:
              path: /index.html
              port: 80
            initialDelaySeconds: 15
            timeoutSeconds: 2
            periodSeconds: 5
            failureThreshold: 1</code></pre></li>
</ul>
<h3 id="commandskubectl--deployments">Commands(kubectl + Deployments)</h3>
<pre><code>    # Create a deployment
    kubectl create -f file.deployment.yml
    # Alternate way to create or apply changes to a 
    # Deployment from YAML
    kubectl apply -f file.deployment.yml
    # Use --save-config when you want to use
    # kubectl apply in the future
    kubectl create -f file.deployment.yml --save-config
    # List all deployments
    kubectl get deployments
    kubectl get deployments --show-labels # Deployments and their labels
    # get all deployments with a specific label
    kubectl get deployments -l app=nginx
    # Delete Deployment (All associated Pods/Containers)
    kubectl delete deployment [deployment-name]</code></pre><h3 id="scaling-pods-horizontally">Scaling Pods Horizontally</h3>
<p>Update the YAML file or use the kubectl scale command</p>
<pre><code>    # Scale the Deployment Pods to 5
    kubectl scale deployment [deployment-name]  --replicas=5

    # Scale by referencing the YAML file
    kubectl scale -f file.deployment.yml --replicas=5

    spec:
      replicas: 3
      selector:
        tier: frontend</code></pre><h4 id="examples">Examples</h4>
<pre><code>    kubectl create -f  nginx.deployment.yml --save-config
    kubectl describe deployment my-nginx
    kubectl get deploy
    kubectl get deployment
    kubectl get deployments
    kubectl get deployments --show-labels
    kubectl get deployments -l app=nginx
    kubectl scale -f nginx.deployment.yml --replicas=3
    kubectl delete -f nginx.deployment.yml</code></pre><h4 id="yaml-file-3">YAML File</h4>
<pre><code>    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: my-nginx
    labels:
      app: my-nginx
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: my-nginx
      template:
        metadata:
          labels:
            app: my-nginx
        spec:
          containers:
          - name: my-nginx
            image: nginx:alpine
            ports:
            - containerPort: 80
            resources:
              limits:
                memory: &quot;128Mi&quot; # 128 MB No spaces between 128 and Mi
                cpu: &quot;200m&quot; # 200 millicpu (0.2 cpu or 20% of the cpu)</code></pre><h3 id="deployment-options">Deployment Options</h3>
<ul>
<li>Zero downtime deployments allow software updates to be deployed to production without impacting end users.</li>
<li>One of the strengths of Kubernetes is zero downtime deployments</li>
<li>Update an applications Pods without impacting end users</li>
<li>Several Options are availabe<ul>
<li>Rolling updates</li>
<li>Blue-green deployments(A&amp;B)(Mutiple environments are running with same environment)</li>
<li>Canary deployments (Very small amount of traffic comes to new version)</li>
<li>Rollbacks</li>
</ul>
</li>
</ul>
<h3 id="rolling-deployments">Rolling Deployments</h3>
<p>If our applications contains 3 replicas of appV1 and if we want roll to appV2, here new pods with appV2 are created one by one and after successful creation of one Pod then one of old pod is removed and this repeated until all desired nodes are created.So there zero downtime in the application.</p>
<p>Update a deployment by changing the YAML and applying changes to the cluster with kubectl apply</p>
<pre><code>    # Apply changes made in a YAML file
    kubectl apply -f file.deployment.yml

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: node-app
    spec:
      replicas: 2
      minReadySeconds: 10
      selector:
        matchLabels:
          app: node-app
      template:
        metadata:
          labels:
            app:node-app
        spec:
          containers:
          - image: node-app:1.0
            name: node-app
            resources:</code></pre><p>Note: minReadySeconds: 10 # waits 10 seconds after container is started ensuring it didnot crash in first 10 seconds to get the traffic</p>
<p><a href="https://github.com/gireeshcse/docker-projects/tree/master/nodejs_apps_to_demonstrate_zero_downtime_kubernetes">Demo Project</a></p>
<h1 id="services">Services</h1>
<ul>
<li>A <strong>Service</strong> provides a single point of entry for accessing one or more Pods.</li>
</ul>
<p>We can&#39;t rely on IP address of Pods because these change a lot.So we need need services since Pods may only live for a short time.</p>
<p>Also Pods can be horizontally scaled so each Pod get its own IP address.</p>
<p>A Pod gets an IP address after it has been scheduled(No way for clients to know IP ahead of time)</p>
<h2 id="role-of-services">Role of services</h2>
<ul>
<li>Services abstract Pod IP addresses from consumers</li>
<li>Load balances between Pods</li>
<li>Relies on labels to associate a service with a Pod.</li>
<li>Node&#39;s <strong>kube-proxy</strong> creates a virtual IP for services</li>
<li>Uses Layer 4 (TCP/UDP over IP)</li>
<li>Services are not ephemeral(Not short lived)</li>
<li>Creates endpoints which sit between a Service and Pod</li>
<li>Services load balances the pods</li>
</ul>
<p>Note: <strong>Once the connection to Pod is established all the user requests will come to this Pod if it is alive.</strong></p>
<h2 id="service-types">Service Types</h2>
<ul>
<li>ClusterIP - Expose the service on a cluster-internal IP (Default)</li>
<li>NodePort  - Expose the service on each Node&#39;s IP at a static port.</li>
<li>LoadBalancer - Provision an external IP to act as a load balancer for the service.</li>
<li>ExternalName - Maps a service to a DNS name</li>
</ul>
<h3 id="clusterip-service">ClusterIP Service</h3>
<ul>
<li>Service IP is exposed internally within the cluster</li>
<li>Only Pods within the cluster can talk to the Service</li>
<li>Allows Pods to talk to other Pods</li>
</ul>
<p><img src="ServiceIP.png" alt="ServiceIP"></p>
<h3 id="nodeport-service">NodePort Service</h3>
<ul>
<li>Exposes the service on each Node&#39;s IP at a static port.</li>
<li>Allocates a port from a range (default is 30000-32767)</li>
<li>Each Node proxies the allocated port.</li>
</ul>
<p>Helpful for testing to reach a particular Pod using NodePort Service.</p>
<img src="NodePort.png" width="100%">

<h3 id="loadbalancer-service">LoadBalancer Service</h3>
<ul>
<li>Exposes a Service externally</li>
<li>Useful when combined with a cloud provider&#39;s load balancer</li>
<li>NodePort and ClusterIP Services are created.</li>
<li>Each Node proxies the allocated port</li>
</ul>
<img src="LoadBalancer.png" width="100%">

<h3 id="externalname-service">ExternalName Service</h3>
<ul>
<li>Service that acts as an alias for an external service</li>
<li>Allows a Service to act as the Proxy for an external service</li>
<li>External service details are hidden from cluster(Easier to change)</li>
</ul>
<img src="ExternalName.png" width="100%">

<h2 id="creating-a-service">Creating a Service</h2>
<pre><code># Listen on port 8080 locally and forward to port 80 in Pod
kubectl port-forward pod/[pod-name]  8080:80

# Listen on port 8080 locally and forward to
kubectl port-forward deployment/[deployment-name] 8080

# Listen on port 8080 locally and forward to Service&#39;s Pod
kubectl port-forward service/[service-name] 8080

# Examples
kubectl port-forward pod/node-app-85dcdf447c-mj6sv 8080:8080
kubectl port-forward deployment/node-app 8080
kubectl port-forward deployment/node-app 9000:8080</code></pre><p><strong>port-forward is used for debugging and testing</strong></p>
<h3 id="defining-services-with-yaml">Defining Services with YAML</h3>
<pre><code>  apiVersion: v1 # Kubernetes API version
  kind: Service  # resource type
  metadata:   # Metadata of the service
  spec:
    type:  # Type of service(ClusterIP,NodePort,LoadBalancer)
           # Defaults to ClusterIP
    selector: # Select Pod template label(s) 
              # that service will apply to

    ports: # Define container target port and the
           # port for the service</code></pre><h4 id="example-clusterip">Example (ClusterIP)</h4>
<pre><code>    apiVersion: v1 # Kubernetes API version
    kind: Service  # resource type
    metadata:   # Metadata of the service
      name: node-app # Name of Service
                     # each service gets a DNS entry
      labels:
        app: node-app
    spec:
      selector:
        app: node-app # Service will apply to the resource 
                      # with a label of app: node-app
      ports:
      - name: http
        port: 8080       # Port of the Service
        targetPort: 8080 # Container Port</code></pre><p>Suppose there are two services with names <strong>frontend</strong> and <strong>backend</strong>. Now a frontend Pod can access a backend Pod using a backend:port</p>
<h4 id="example-nodeport">Example (NodePort)</h4>
<pre><code>    apiVersion: v1 # Kubernetes API version
    kind: Service  # resource type
    metadata:   # Metadata of the service
      name: node-app # Name of Service
                     # each service gets a DNS entry
      labels:
        app: node-app
    spec:
      selector:
        type: NodePort
        app: node-app # Service will apply to the resource 
                      # with a label of app: node-app
      ports:
      - name: http
        port: 8080       # Port of the Service
        targetPort: 8080 # Container Port
        nodePort: 31000  # Optionally set b/w 30000-32767</code></pre><p>Note: I can access the service using 127.0.0.1:31000 by not localhost:31000(Ubuntu 18.04)</p>
<h4 id="example-loadbalancer">Example (LoadBalancer)</h4>
<pre><code>    apiVersion: v1 # Kubernetes API version
    kind: Service  # resource type
    metadata:   # Metadata of the service
      name: node-app # Name of Service
                     # each service gets a DNS entry
      labels:
        app: node-app
    spec:
      selector:
        type: LoadBalancer  # normally used  with cloud providers
        app: node-app # Service will apply to the resource 
                      # with a label of app: node-app
      ports:
      - port: 8080       # Port of the Service defaults to 80
        targetPort: 8080 # Container Port</code></pre><h4 id="example-externalname">Example (ExternalName)</h4>
<ul>
<li><p>Other Pods can use this FQDN to access the external service</p>
</li>
<li><p>Service will proxy to FQDN</p>
<pre><code>  apiVersion: v1 # Kubernetes API version
  kind: Service  # resource type
  metadata:   # Metadata of the service
    name: external-service
    labels:
      app: node-app
  spec:
    selector:
      type: ExternalName
      externalName: api.acmecorp.com
    ports:
    - port: 9000       # Port of the Service defaults to 80</code></pre></li>
</ul>
<h3 id="commands">Commands</h3>
<pre><code>  # Creates CusterIP if we have not specified type
  kubectl create -f file.service.yml 

  # Update a Service
  # Assumes --save-config was used with create
  kubectl apply -f file.service.yml

  # Shell into a Pod and test a URL. Add -c [containerID]
  # in cases where multiple containers are running in the Pod
  kubectl exec [pod-name]  -- curl -s http://podIP

  # Install and use curl (example Alpine Linux)
  kubectl exec [pod-name] -it sh
  &gt; apk add curl
  &gt; curl -s http://podIP</code></pre><h3 id="example-standalonepodyml">Example standalone.pod.yml</h3>
<pre><code>  apiVersion: v1
  kind: Pod
  metadata:
    name: node-app
  spec:
    containers:
    - name: node-app
      image: gireeshcse/node-demo-k8:v3

  kubectl create -f standalone.pod.yml --save-config
  kubectl get pods
  # Output of above command first one is standalone Pod
  NAME                        READY   STATUS    RESTARTS   AGE
  node-app                    1/1     Running   0          6m9s
  node-app-85dcdf447c-mj6sv   1/1     Running   0          108m
  node-app-85dcdf447c-x7492   1/1     Running   0          108m
  # to Get IP of pod
  kubectl get pod node-app-85dcdf447c-mj6sv -o yaml
  kubectl describe pod node-app-85dcdf447c-mj6sv
  # The above commands output contains
  # podIP: 172.17.0.6  # First Command
  # IP: 172.17.0.6  # 2nd Command

  # open shell prompt in standalone Pod 
  kubectl exec node-app -it sh
  # uname -a
  Linux node-app 5.0.0-29-generic #31~18.04.1-Ubuntu SMP Thu Sep 12 18:29:21 UTC 2019 x86_64 GNU/Linux
  # curl http://172.17.0.6:8080
  Hello World! Version1 -- node-app-85dcdf447c-mj6sv

  # clusterIP.service.yml

  apiVersion: v1
  kind: Service
  metadata:
    name: nodeapp-clusterip
  spec:
    type: ClusterIP
    selector:
      app: node-app
    ports:
    - port: 8001
      targetPort: 8080

  kubectl apply -f clusterIP.service.yml
  kubectl get services
  # Output
  NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
  kubernetes          ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    71d
  nodeapp-clusterip   ClusterIP   10.96.251.157   &lt;none&gt;        8001/TCP   52s

  # open shell prompt in standalone Pod 
  kubectl exec node-app -it sh
  # curl 10.96.251.157:8001
  Hello World! Version1 -- node-app-85dcdf447c-x7492
  # curl http://nodeapp-clusterip:8001
  Hello World! Version1 -- node-app-85dcdf447c-x7492

  kubectl exec node-app-85dcdf447c-x7492 -it sh
  # curl http://nodeapp-clusterip:8001
  Hello World! Version1 -- node-app-85dcdf447c-mj6sv

  kubectl get pods
  NAME                        READY   STATUS    RESTARTS   AGE
  node-app                    1/1     Running   0          31m
  node-app-85dcdf447c-mj6sv   1/1     Running   0          133m
  node-app-85dcdf447c-x7492   1/1     Running   0          133m

  # Delete the service
  kubectl delete -f clusterIP.service.yml</code></pre><p>The above demonstrates pods calling pods. Used for debugging</p>
<h3 id="nodeport">NodePort</h3>
<pre><code>  # standalone.pod.yml

  apiVersion: v1
  kind: Pod
  metadata:
    name: node-app-standalone
  spec:
    containers:
    - name: node-app-standalone
      image: gireeshcse/node-demo-k8:v3

  kubectl create -f standalone.pod.yml --save-config

  # nodeport.service.yml

  apiVersion: v1
  kind: Service
  metadata:
    name: nodeapp-nodeport
  spec:
    type: NodePort
    selector:
      app: node-app
    ports:
    - port: 8080
      targetPort: 8080
      nodePort: 31000

  kubectl apply -f nodeport.service.yml
  kubectl get services
  # O/p of above command
  NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
  kubernetes         ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          71d
  nodeapp-nodeport   NodePort    10.96.201.120   &lt;none&gt;        8001:31000/TCP   38s

  # Worked for
  http://127.0.0.1:31001/
  # http://localhost:31001 didn&#39;t work

  # Delete
  kubectl delete -f nodeport.service.yml</code></pre><p>Here it is exposed to the outside the cluster.</p>
<h3 id="loadbalancer">LoadBalancer</h3>
<pre><code>  # loadbalancer.service.yml
  apiVersion: v1
  kind: Service
  metadata:
    name: nodeapp-loadbalancer
  spec:
    type: LoadBalancer
    selector:
      app: node-app
    ports:
    - name: &quot;8002&quot;
      port: 8002
      targetPort: 8080

  kubectl create -f loadbalancer.service.yml --save-config

  kubectl get services
  # Output
  NAME                           TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
  service/kubernetes             ClusterIP      10.96.0.1     &lt;none&gt;        443/TCP          71d
  service/nodeapp-loadbalancer   LoadBalancer   10.96.9.158   &lt;pending&gt;     8080:31309/TCP   3m58s

  # For Docker Destop EXTERNAL-IP of load-balancer will be localhost
  # You can access localhost:8080

  # For Minikube
  sudo minikube service nodeapp-loadbalancer
  |-----------|----------------------|-------------|--------------------------|
  | NAMESPACE |         NAME         | TARGET PORT |           URL            |
  |-----------|----------------------|-------------|--------------------------|
  | default   | nodeapp-loadbalancer |        8080 | http://192.168.1.4:31309 

  # to access http://192.168.1.4:31309</code></pre><h1 id="storage-options">Storage Options</h1>
<ul>
<li><p>To store application state/data and exchange it between Pods with Kubernetes -- We need <strong>Volumes</strong>.</p>
</li>
<li><p>A <strong>Volume</strong> can be used to hold data and state for Pods and containers.</p>
</li>
<li><p>A Pod can have multiple Volumes attached to it.</p>
</li>
<li><p>Containers rely on a mountPath to access a Volume.</p>
</li>
<li><p>Kubernetes suppots:</p>
<ul>
<li>Volumes</li>
<li>PersistentVolumes</li>
<li>PersistentVolumeClaims</li>
<li>StorageClasses</li>
</ul>
</li>
<li><p>A Volume references a storage location</p>
</li>
<li><p>Must have a unique name</p>
</li>
<li><p>Attached to a Pod and may or may not be tied to the Pod&#39;s lifetime (depending on the Volume type)</p>
</li>
<li><p>A Volume Mount references a Volume by name and defines a mountPath</p>
</li>
</ul>
<h2 id="volumes-type-examples">Volumes Type Examples</h2>
<ul>
<li><strong>emptyDir</strong>- Empty directory for storing transient data(shares a Pod&#39;s lifetime) useful for sharing files between containers running in a Pod.</li>
<li><strong>hostPath</strong>- Pod mounts into the nodes filesystem.</li>
<li><strong>nfs</strong> - an NFS(Network file System) share mounted into the Pod</li>
<li><strong>configMap/secret</strong> - Special types of voumes that provide a Pod with access to Kubernetes resources.</li>
<li><strong>persistentVolumeClaim</strong> - Provides Pods with a more persistent storage option that is abstracted from the details.</li>
<li><strong>Cloud</strong> - Cluster-wide storage.</li>
</ul>
<h3 id="defining-an-emptydir-volume">Defining an emptyDir Volume</h3>
<pre><code>  apiVersion: v1
  kind: Pod
  metadata:
    name: nginx-alpine-volume
  spec:
    volumes:
      - name: html # Define Initial Volume 
        emptyDir: {} # Lifetime of the Pod
    containers:
    - name: nginx
      image: nginx:alpine
      volumeMounts:
        - name: html # Reference &quot;html&quot; Volume
          mountPath: /usr/share/nginx/html
          readOnly: true
    - name: html-updater
      image: alpine
      command: [&quot;bin/sh&quot;,&quot;-c&quot;]
      args: # updates latest date every 10 sec
        - while true; do date &gt;&gt; /html/index.html;
            sleep 10; done
      volumeMounts:
        - name: html # Reference &quot;html&quot; Volume
          mountPath: /html

  # Commands
  kubectl create -f emptydir.pod.yml --save-config
  kubectl port-forward nginx-alpine-volume 8080:80</code></pre><p>2 containers one is creating and other is displaying</p>
<h3 id="defining-an-hostpath-volume">Defining an hostPath Volume</h3>
<pre><code>    apiVersion: v1
    kind: Pod
    metadata:
      name: docker-volume
    spec:
      volumes:
        - name: docker-socket # Define a socket volume
          hostPath: # on host
            path: /var/run/docker.sock
            type: Socket
      containers:
      - name: docker
        image: docker
        command: [&quot;sleep&quot;]
        args: [&quot;100000&quot;]
        volumeMounts:
          - name: docker-socket # Reference &quot;html&quot; Volume
            mountPath: /usr/share/nginx/html</code></pre><p>Valid types include </p>
<ul>
<li>DirectoryOrCreate</li>
<li>Directory</li>
<li>FileOrCreate</li>
<li>File</li>
<li>Socket</li>
<li>CharDevice</li>
<li>BlockDevice</li>
</ul>
<pre><code>    #Commands
    kubectl create -f hostpath.pod.yml --save-config

    kubectl describe pod docker-volume
    #Contains following info
    Volumes:
    docker-socket:
      Type:          HostPath (bare host directory volume)
      Path:          /var/run/docker.sock
      HostPathType:  Socket
    default-token-g5sp9:
      Type:        Secret (a volume populated by a Secret)
      SecretName:  default-token-g5sp9
      Optional:    false

    kubectl exec docker-volume -it sh</code></pre><p>The volumes key should be at the same level as containers (In deployments file)</p>
<h3 id="cloud-volumes">Cloud Volumes</h3>
<ul>
<li>Azure - Azure Disk and Azue File</li>
<li>AWS - Elastic Block Store</li>
<li>GCP - GCE Persistent Disk</li>
</ul>
<h4 id="azurepodyml">azure.pod.yml</h4>
<pre><code>    apiVersion: v1
    kind: Pod
    spec:
      volumes:
        - name: data
          azureFile:
            secretName: &lt;azure-secret&gt;
            shareName: &lt;share-name&gt;
            readOnly: false
      containers:
      - name: my-app
        image: someimage
        volumeMounts:
          - name: data # Reference &quot;data&quot; Volume
            mountPath: /data/storage</code></pre><h4 id="awspodyml">aws.pod.yml</h4>
<pre><code>    apiVersion: v1
    kind: Pod
    spec:
      volumes:
        - name: data
          awsElasticBlockStore:
            volumeID: &lt;volume_ID&gt;
            fsType: ext4
      containers:
      - name: my-app
        image: someimage
        volumeMounts:
          - name: data # Reference &quot;data&quot; Volume
            mountPath: /data/storage</code></pre><h4 id="gcppodyml">gcp.pod.yml</h4>
<pre><code>      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: data
            gcePersistentDisk:
              pdName: datastorage
              fsType: ext4
        containers:
        - name: my-app
          image: someimage
          volumeMounts:
            - name: data # Reference &quot;data&quot; Volume
              mountPath: /data/storage

  # To see info about volumes
  kubectl describe pod [pod-name]
  kubectl get pod [pod-name] -o yaml</code></pre><h3 id="persistentvolumepv">PersistentVolume(PV)</h3>
<ul>
<li><p>A <strong>PersistentVolume(PV)</strong> is a cluster-wide storage unit provisioned by an administrator with a lifecycle independent from a Pod.</p>
</li>
<li><p>A <strong>PersistentVolumeClaim(PVC)</strong> is a request for a storage unit (PV).</p>
</li>
<li><p>A PersistentVolume is a cluster-wide storage resource that relies on network-attached storage(NAS)</p>
</li>
<li><p>Normally provisioned by a cluster administrator</p>
</li>
<li><p>Available to a Pod even if it gets rescheduled to a different Node</p>
</li>
<li><p>Rely on a storage provider such as NFS, cloud storage, or other options</p>
</li>
<li><p>Associated with a Pod using a PersistentVolumeClaim(PVC)</p>
</li>
</ul>
<h4 id="azurepersistentvolumeyml">azure.persistentvolume.yml</h4>
<pre><code>    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: my-pv
    spec:
      capacity: 10Gi
      accessModes:
        - ReadWriteOnce # One client can mount for read/write
        - ReadOnlyMany # Many clients can mount for reading 
      persistentVolumeReclaimPolicy: Retain 
      # Retain even after claim is deleted
      azureFile:
        secretName: &lt;azure-secret&gt;
        shareName: &lt;name_from_azure&gt;
        readOnly: false</code></pre><h4 id="pesistentvolumeclaimyml">pesistentvolumeclaim.yml</h4>
<pre><code>    kind: PesistentVolumeClaim
    apiVersion: v1
    metadata:
      name: pv-dd-account-hdd-5g
      annotations:
        volume.beta.kubernetes.io/storage-class: accounthdd
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi</code></pre><h4 id="persistentclaimpodyml">persistentclaim.pod.yml</h4>
<pre><code>    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-uses-account-hdd-5g
      labels:
        name: storage
    spec:
      containers:
      - name: az-c-01
        image: nginx
        command: [&quot;bin/sh&quot;,&quot;-c&quot;]
        args: # updates latest date every 1 sec
          - while true; do echo $(date) &gt;&gt; /mnt/blobdisk/outfile;
              sleep 1; done
        volumeMounts:
          - name: blobdisk01 # Reference &quot;blobdisk01&quot; Volume
            mountPath: /mnt/blobdisk
      volumes:
        - name: blobdisk01 # Define  Volume 
          persistentVolumeClaim:
            claimName: pv-dd-account-hdd-5g</code></pre><h2 id="storage-class">Storage Class</h2>
<ul>
<li>A <strong>StorageClass(SC)</strong> is a type of storage template that can be used to dynamically provision storage.</li>
<li>Used to define different classes of storage</li>
<li>Act as a type of storage template</li>
<li>Supports dynamic provisioning of PersistentVolumes</li>
<li>Adminstrators don&#39;t have to create PVs in advance.</li>
</ul>
<h3 id="define-localstorage">Define LocalStorage</h3>
<pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
reclaimPolicy: Retain # Default delete after PVC is released
provisioner: kubernetes.io/no-provisioner # Provisioner Used to create PV resource
volumeBindingMode: WaitForFirstConsumer 
#Wait to create until Pod making PVC is created
#Default is immediate(ceate once PVC is created)</code></pre><h3 id="defining-local-storage-pv">Defining Local Storage PV</h3>
<pre><code>  apiVersion: v1
  kind: PesistentVolume
  metadata:
    name: my-pv
  spec:
    capacity:
      storage: 10Gi
    volumeMode: Block
    accessModes:
    - ReadWriteOnce
    storageClassName: local-storage # Reference Storage class
    local:
      path: /data/storage # Path where data is stored on Node
    nodeAffinity:
      required:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - &lt;node-name&gt;</code></pre><p>In above, It selects the Node where the local storage PV is created based on conditions</p>
<h3 id="pesistentvolume-claim">PesistentVolume Claim</h3>
<pre><code>    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: my-pvc
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: local-storage
      resources:
        requests:
          storage: 1Gi</code></pre><h3 id="using-pvc">Using PVC</h3>
<pre><code>  apiVersion: apps/v1
  kind: [Pod | Deployment | StatefulSet]

  ...
    spec:
      volumes:
      - name: my-volume
        persistentVolumeClaim:
          claimName: my-pvc</code></pre><h3 id="kubernetes-statefulset">Kubernetes StatefulSet</h3>
<p>Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.</p>
<p>Like a Deployment , a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p>
<p>StatefulSets are valuable for applications that require one or more of the following.</p>
<ul>
<li>Stable, unique network identifiers.</li>
<li>Stable, persistent storage.</li>
<li>Ordered, graceful deployment and scaling.</li>
<li>Ordered, automated rolling updates.</li>
</ul>
<h1 id="configmaps-and-secrets">ConfigMaps and Secrets</h1>
<ul>
<li>Provides a way to a store cofiguration info and provide it to containers.</li>
<li>Provides a way to inject configuration data into a container.</li>
<li>Can store entire files or provide key/value pairs.</li>
</ul>
<p>Access from Pod</p>
<ul>
<li>Environment variables(Key/Value)</li>
<li>ConfigMap volume (Access as files)</li>
</ul>
<p>apiVersion: v1
kind: ConfigMap
metadata:
  name: app-settings
  labels:
    name: app-settings</p>
<h1 id="credits">Credits</h1>
<p><a href="https://github.com/kubernetes/examples">Great Site </a></p>
<p><a href="https://github.com/DanWahlin/CodeWithDanDockerServices/">Main Credit</a></p>
